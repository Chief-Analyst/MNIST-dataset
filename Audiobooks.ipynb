{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6a3875b-3dcd-4f83-8ca2-ececf4be5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import relevant libraries \n",
    "\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48d81eef-439f-47ab-9601-c6aa4fdc5fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load dataset\n",
    "raw_csv_data = np.loadtxt(\"C:/Users/Administrator/Downloads/Audiobooks_data.csv\", delimiter = ',')\n",
    "\n",
    "unscaled_inputs_all = raw_csv_data[:, 1:-1]\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "daf26082-6713-452c-8e5b-5c5cdafbbbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Balance the dataset\n",
    "\n",
    "num_one_targets = int(np.sum(targets_all))\n",
    "zero_targets_counter = 0\n",
    "indices_to_remove = []\n",
    "\n",
    "for i in range (targets_all.shape[0]):\n",
    "    if targets_all[i] ==0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis = 0)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7f9a5761-2d99-4540-9b4e-f018fa02914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Standardize inputs using sklearn\n",
    "\n",
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d7aaa2b3-e086-4a0a-ac35-2ded700c1d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = scaled_inputs[:targets_equal_priors.shape[0], :]\n",
    "shuffled_indices = np.arange(targets_equal_priors.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ddbbee3d-38ce-4f2c-a39d-752935c47dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  1789.0 3578 0.5\n",
      "Validation set:  224.0 448 0.5\n",
      "Test set:  224.0 448 0.5\n"
     ]
    }
   ],
   "source": [
    "##Split dataset into train, validation and test\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppose shuffled_inputs and shuffled_targets are your full dataset\n",
    "# Example:\n",
    "# shuffled_inputs = np.array([...])\n",
    "# shuffled_targets = np.array([...])\n",
    "\n",
    "# First, split off the test set (10% of total samples)\n",
    "train_val_inputs, test_inputs, train_val_targets, test_targets = train_test_split(\n",
    "    shuffled_inputs, shuffled_targets, test_size=0.1, \n",
    "    stratify=shuffled_targets, random_state=42\n",
    ")\n",
    "\n",
    "# Now, split the remaining 90% into train (80%) and validation (10% of total)\n",
    "# Validation fraction relative to train_val set:\n",
    "val_fraction = 0.1111  # 0.1 / 0.9 ≈ 11.11%\n",
    "\n",
    "train_inputs, validation_inputs, train_targets, validation_targets = train_test_split(\n",
    "    train_val_inputs, train_val_targets, test_size=val_fraction, \n",
    "    stratify=train_val_targets, random_state=42\n",
    ")\n",
    "\n",
    "# Check sizes and fractions of 1s\n",
    "print(\"Train set: \", np.sum(train_targets), train_targets.shape[0], np.sum(train_targets)/train_targets.shape[0])\n",
    "print(\"Validation set: \", np.sum(validation_targets), validation_targets.shape[0], np.sum(validation_targets)/validation_targets.shape[0])\n",
    "print(\"Test set: \", np.sum(test_targets), test_targets.shape[0], np.sum(test_targets)/test_targets.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ca0ffb25-2207-4b40-88fe-84ebd7b0d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save all three datasets in .npz format\n",
    "\n",
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d1e165d5-1da6-4a7d-8c0f-e05cbf6ac3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Time to build the model using tensorflow\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2fcd4496-b896-44cc-8fac-f533518c1ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load the three .npz datasets\n",
    "\n",
    "npz = np.load(\"C:/Users/Administrator/Audiobooks_data_train.npz\")\n",
    "\n",
    "train_inputs = npz['inputs'].astype(np.float64)\n",
    "train_targets = npz['targets'].astype(np.int32)\n",
    "\n",
    "npz = np.load(\"C:/Users/Administrator/Audiobooks_data_validation.npz\")   \n",
    "\n",
    "validation_inputs = npz['inputs'].astype(np.float64)\n",
    "validation_targets = npz['targets'].astype(np.int32)\n",
    "\n",
    "npz = np.load(\"C:/Users/Administrator/Audiobooks_data_test.npz\")   \n",
    "\n",
    "test_inputs = npz['inputs'].astype(np.float64)\n",
    "test_targets = npz['targets'].astype(np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4479a0b5-336a-4161-b26a-28ba89a1e4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "112/112 - 1s - 12ms/step - accuracy: 0.7457 - loss: 0.4587 - val_accuracy: 0.8259 - val_loss: 0.3321\n",
      "Epoch 2/100\n",
      "112/112 - 0s - 2ms/step - accuracy: 0.7937 - loss: 0.3748 - val_accuracy: 0.7991 - val_loss: 0.3227\n",
      "Epoch 3/100\n",
      "112/112 - 0s - 2ms/step - accuracy: 0.8046 - loss: 0.3557 - val_accuracy: 0.8482 - val_loss: 0.2991\n",
      "Epoch 4/100\n",
      "112/112 - 0s - 2ms/step - accuracy: 0.8007 - loss: 0.3535 - val_accuracy: 0.8504 - val_loss: 0.2964\n",
      "Epoch 5/100\n",
      "112/112 - 0s - 2ms/step - accuracy: 0.8004 - loss: 0.3487 - val_accuracy: 0.8147 - val_loss: 0.3139\n",
      "Epoch 6/100\n",
      "112/112 - 0s - 2ms/step - accuracy: 0.8133 - loss: 0.3398 - val_accuracy: 0.8549 - val_loss: 0.2832\n",
      "Epoch 7/100\n",
      "112/112 - 0s - 2ms/step - accuracy: 0.8083 - loss: 0.3431 - val_accuracy: 0.8371 - val_loss: 0.3081\n",
      "Epoch 8/100\n",
      "112/112 - 0s - 2ms/step - accuracy: 0.8102 - loss: 0.3385 - val_accuracy: 0.8571 - val_loss: 0.2963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1bdff891550>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Model\n",
    "\n",
    "input_size= 10\n",
    "output_size = 2\n",
    "hidden_layer_size = 100\n",
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                             tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                             tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "                            ])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "max_epochs = 100\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "model.fit(train_inputs, train_targets,\n",
    "         batch_size = batch_size, epochs = max_epochs,\n",
    "          callbacks = early_stopping,\n",
    "         validation_data=(validation_inputs, validation_targets),\n",
    "         verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "02887118-1a30-4b14-af0a-728100994cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8504 - loss: 0.3199 \n"
     ]
    }
   ],
   "source": [
    "##Testing the model\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901716e6-d4c9-471c-a58b-8d8fb5c092ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
