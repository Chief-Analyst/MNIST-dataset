{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef1ac637-4541-4819-9d9f-2d82923714e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import relevant packages\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2920ebce-7a93-410a-808c-cdac6de2370e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-datasets in c:\\users\\administrator\\anaconda3\\lib\\site-packages (4.9.9)\n",
      "Requirement already satisfied: absl-py in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (2.3.1)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (0.1.9)\n",
      "Requirement already satisfied: etils>=1.9.1 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (1.13.0)\n",
      "Requirement already satisfied: immutabledict in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (4.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (2.1.3)\n",
      "Requirement already satisfied: promise in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (5.29.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (5.9.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (19.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (2.32.3)\n",
      "Requirement already satisfied: simple_parsing in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (0.1.7)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.17.2)\n",
      "Requirement already satisfied: termcolor in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (3.2.0)\n",
      "Requirement already satisfied: toml in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (4.67.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tensorflow-datasets) (1.17.0)\n",
      "Requirement already satisfied: einops in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (0.8.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (2025.3.2)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (6.5.2)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (4.12.2)\n",
      "Requirement already satisfied: zipp in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2025.8.3)\n",
      "Requirement already satisfied: attrs>=18.2.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from dm-tree->tensorflow-datasets) (24.3.0)\n",
      "Requirement already satisfied: six in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from promise->tensorflow-datasets) (1.17.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from simple_parsing->tensorflow-datasets) (0.17.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tensorflow-metadata->tensorflow-datasets) (1.71.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tqdm->tensorflow-datasets) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f10dd615-9406-437a-974a-f9584929dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "348829a6-306a-48f4-813b-0731a72ea9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data chosen is MNIST\n",
    "\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "122b9bba-58e1-4331-90ae-873e3aa7a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split dataset into train, validation and test datasets\n",
    "\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "##No validation split in the original dataset which means I have to split myself. I will use 10% of the train data as validation dataset\n",
    "##Note that mnist_info.splits splits the dataset based on the info contained in the dataset(here: no of samples)\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "\n",
    "##The tf.cast function helps round up the results from splits into an int just incase it is a float\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "##Do same for test dataset\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "##Scale inputs\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "##Shuffle dataset in batches. I set buffer size to 10000\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "##Batch train data cos of backward propagation using '.batch()' function\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "##To reshape the validation dataset\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8215fc3-ec76-471b-962c-055dc24e1513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "##Outline the model\n",
    "\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size= 200\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax'),\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28f522ef-dba4-4495-ac44-94d2e116ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Choose the optimizer and the loss function\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f81e9bb8-d53c-4dd3-a84c-f45254e1ed16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 6s - 11ms/step - accuracy: 0.9204 - loss: 0.2672 - val_accuracy: 0.9628 - val_loss: 0.1249\n",
      "Epoch 2/5\n",
      "540/540 - 4s - 8ms/step - accuracy: 0.9689 - loss: 0.1004 - val_accuracy: 0.9765 - val_loss: 0.0808\n",
      "Epoch 3/5\n",
      "540/540 - 4s - 8ms/step - accuracy: 0.9783 - loss: 0.0695 - val_accuracy: 0.9727 - val_loss: 0.0904\n",
      "Epoch 4/5\n",
      "540/540 - 4s - 7ms/step - accuracy: 0.9837 - loss: 0.0520 - val_accuracy: 0.9843 - val_loss: 0.0548\n",
      "Epoch 5/5\n",
      "540/540 - 4s - 7ms/step - accuracy: 0.9862 - loss: 0.0422 - val_accuracy: 0.9895 - val_loss: 0.0413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x27c1bb4ecf0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Training the model\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236065b1-4a03-432a-ba7e-09eb9440da55",
   "metadata": {},
   "source": [
    "To improve accuracy of the validation dataset (which is the deciding factor here), I switched the hidden layer size from 50 to 100 to 200, and the added one more hidden layer and found increase in accuracy than the previous trainings. This further reflects and helps me realize how important these hyperparaneters and activations are to a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6649bec3-7a99-4b62-acf9-4e093e10284c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step - accuracy: 0.9799 - loss: 0.0674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06740030646324158, 0.9799000024795532]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a3eb1-f07e-48e6-a11a-27c21aeb56db",
   "metadata": {},
   "source": [
    "97% accuracy on test data!!! Not bad!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
